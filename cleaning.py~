#!/usr/bin/env python
# coding: utf-8

# In[4]:


import spacy
import re
from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop



# In[5]:


#le nom du fichier contenant le dataset
dataset = "dataset_full" 

#le nom du fichier contenant la stoplist
stoplist = "stoplist.txt"

#le nom du fichier dans lequel le dataset nettoyé sera sauvegardé
clean_dataset = "clean_dataset_full"


# In[6]:


with open(stoplist, "r") as f:
    stopwords = f.read().splitlines()


# In[7]:


#importer le vocabulaire français
nlp = spacy.load("fr_core_news_sm")
#nlp = spacy.load('fr')


# In[13]:


#Récupération des tweets
with open(dataset, "r") as file:
    raw_data = file.readlines()


# In[ ]:


#transformer les tweets en objet spacy
docs = list(nlp.pipe(raw_data))


# In[6]:


#tokeniser et lemmatiser l\'ensemble des tweets
""" -enlever la ponctuation,
    - enlever les espaces
    -enlever les email
    -enlever les urls"""

first_clean_data = [[token.lemma_.lower() for token in doc                               if not token.is_punct and not token.is_space and not token.like_url and not token.like_email and token.tag_!= 'Num'] for doc in docs]


# In[7]:


#Deuxième passe de nettoyage
def clean(msg):
    clean_msg = []
    for word in msg:
        #Supprimer tout ce qui n'est pas des lettres
        clean_word = re.sub(r'\W', '', word)
        #Supprimer les chiffres
        clean_word = re.sub(r'\d', '', clean_word)
        if clean_word not in stopwords : 
            clean_msg.append(clean_word)
    return clean_msg


# In[8]:


clean_data = []
#Nettoyage des tweets
for msg in first_clean_data:
    clean_data.append(clean(msg))


# In[80]:


#On a bien uniquement du texte
print(raw_data[0], end='')
print(first_clean_data[0])
print(clean_data[0])


# In[81]:


#enregistrer sur le disque
with open(clean_dataset, "w") as f:
    for msg in clean_sample:
        f.write(" ".join(msg) + '\n')


# In[ ]:




